# realallocator_full
Code for data analysis, development, and preprocessing for RealAllocator app, which provides a recommendation of how to rebalance a two asset portfolio (equities and bonds) into a three asset portfolio (equities, bonds, and direct real estate) and provides backtest results to compare the current and realallocator portfolios. This is a project done in consultation with [RealCrowd](https://www.realcrowd.com), a direct real estate crowd funding company.

Realallocator can be used [here](https://realallocator.herokuapp.com) with code in `app` directory of this repo.

## Overview of Data Pipeline
RealCrowd provided private data from NCREIF National Properties Index (NPI) about the returns of private commercial real estate assets. The data had quarterly returns grouped by property type (retail, office, industrial, etc.) and metropolitian statistical area. They also provided quarterly returns for the S&P500, some bond indices, and 90-day treasury bills for comparison. I collapsed the NPI data down and merged with S&P500, US Government bonds (some missing data from what NCREIF supplied, based on a Barclays index, was filled in using iShares GOVT data), and the treasury bill returns (for "risk-free" return information).

I also created a dataset of possible portfolios, in 1% allocation increments, of the three asset classes. The total allocation for each portfolio is 100% and each asset class must be invested in. Based on investigation into two asset portfolio allocations used by various financial organizations and discussions with RealCrowd, I devised bounds on equity and bond allocations for various risk tolerances (5 levels to match RealCrowd's risk assessment tool).  The low variation in real estate returns, combined with historical average returns higher than bonds made real estate too appealing for optimization techniques without these bounds (at least one of equities or bonds would be given 1% allocation). Each risk tolerance category is allowed to have 23% in real estate under these bounds.

## Overview of Algorithm
Since returns of financial markets are notoriously un-predictable, and using only snippets of time can cause dramatic variation in portfolio return and volatility calculations, I pursued a Bayesian model for sampling returns from a posterior predictive distribution that was based on 40 years of historic data as well as reasonable assumption about the structure of asset returns. To account for market conditions, I used spectral clustering to group the 40 years of quarterly return data into 3 groups, one was essentially bull markets, another bear markets, and the third captured most quarters which see very slight negative to moderate (~7%) returns in equities. Using [PyMC3](https://docs.pymc.io), I built a model for the posterior distribution of returns in each of the 3 market clusters, and then obtained 100 samples from these distributions (returns and volatility of 4 assets: equities, bonds, real estate, and T-bills, under each of 3 market conditions). The return and volatility was then used to compute a average Sharpe ratio for the sample from a particular set of market conditions for each portfolio (averaging over the 100 sample returns of each type of asset). These average Sharpe ratios for each portfolio within each market condition where averaged across market conditions (considering future market conditions equally like to have high, moderate, or low/negative growth). The portfolio with highest average Sharpe ratio, subject to the bounds on equity and bond allocation for a particular risk tolerance is then selected.

For the web app, the clustering, Bayesian posterior construction, sampling, and computation of Sharpe ratios are all done once with results stored to a file. The app then just needs to do a table look-up and calculate backtest results. This was done primarily for performance and because the Bayesian posterior will only need to be updated once per quarter, the data is very static. Backtest data also only changes each quarter, but requires the current portfolio allocation which can change with each use of the app, so it had to be done live.

## Description of Repository Contents
Most files have comments and detailed descriptions, here I only lay-out each files place in the grand scheme of the app.
* `mk-portfolio-all.py` and `mk-portfolio-risk-valid.py` generate sets of possible portfolios, the latter only creates portfolios subject to risk tolerances which was ultimately not used due to a desire to make the bounds on equities and bonds more modular and not wanting to re-build the portfolio set with an small change.
* `port-sim-w-rf-ret-out.py` does the collapsing and merging of NPI data with other market data. It is intended to be used as a notebook rather than a script. It does output the main return data set for later use.
* `cluster-market-cond.py` assigns cluster labels to the quarterly return data.
* `assign-sharpe.ipynb` is a notebook that builds the Bayesian model, assigns Sharpe ratio to each portfolio for a specific cluster, and saves resulting portfolio set. This needs to be run for each cluster and wasn't automated to allow for interactive convergence checks of the MCMC.
* `join-sharpes.ipynb` merges the individual cluster results in the last step into a single file. This is what is used by the app.

The following files aren't directly used by the app and reflect other analysis done.
* `npi-explore.py` is code to explore the NPI data. Notebook style.
* `mvp-demo.ipynb` Contains an approach taken initially to use quantiles on the observed risk to divide portfolios into groups. This is an alternative to the bounds, but is highly dependent on market conditions and almost always results in portfolios that are unrealistic (98/1/1 allocations).
* `ret-slide1.ipynb` and `returns-by-cluster.ipynb` are largely to provide graphs for demo slides.
* `mcmc-opt.py` is an attempt to use a Monte Carlo approach to explore the portfolio Sharpe ratio space. It still needs something similar to the bounds used, produced similar results, but had a tendency to get stuck in infinite loops. This didn't seem promising to pursue given time constraints.
* `validator.ipynb` contains code used to validate the model.
